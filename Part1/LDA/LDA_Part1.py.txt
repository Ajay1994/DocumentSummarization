############LDA PART 1 ###############

# -*- coding: utf-8 -*-
"""
Created on Sat Apr 16 21:49:44 2016

@author: KRUNAL
"""

import os
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
import pickle
from gensim import corpora, models
import gensim
from nltk.tokenize import sent_tokenize, word_tokenize
import numpy as np
from scipy import spatial
import math
from scipy import linalg as la
import pandas as pd


## for tokenizing , and removing stopwords
tokenizer = RegexpTokenizer(r'\w+')
en_stop = get_stop_words('en')
p_stemmer = PorterStemmer()


##take list of files of DUC01 dataset
txtFiles = []
for root, dirs, files in os.walk("./DUC01"):
    for file in files:
        if file.endswith(".body"):
            file = os.path.join(root, file)
            print(file)
            txtFiles.append(file)
            
##write file list             
files = open("./filelist.txt", 'w')
for file in txtFiles:
    filename = file.split("/")
    filename = filename[len(filename)-1]
    files.write(filename)
    files.write("\n")

files.close()

##get all texts for training on lda model
texts = []
for file in txtFiles :
    with open(file, 'r') as myfile:
        raw = myfile.read().replace('\n', '')
    tokens = tokenizer.tokenize(raw)
    stopped_tokens = [i for i in tokens if not i in en_stop]
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    texts.append(stemmed_tokens)

##dump stemmed tokens for future use,
with open('stemmed_tokenssmall.txt', 'wb') as f:
   pickle.dump(texts, f)


## make dictionary and corpus for ldamodel to training
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

##train ldamodel for desired no of topics
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=100)
with open('lda10.pkl', 'wb') as f:
   pickle.dump(ldamodel, f)


##load in case ldamodel has changed
##pkl_file = open('lda10.pkl', 'rb')
##ldamodel = pickle.load(pkl_file)
##pkl_file.close()

print(ldamodel.print_topics(num_topics=10, num_words=30))

##take feture matrix
count = 1
#fileList = txtFiles[1:5]
for file in txtFiles:
    print("---------------------------------------------------------------------")
    print(count," : ", file)
    print("---------------------------------------------------------------------")
    with open(file, 'r') as myfile:
        data=myfile.read().replace('\n', '')

    #Tokenize into sentences    
    sentences = sent_tokenize(data)
    #Creating a matrix to hold the sentence vectors of the doucment
    sent_matrix = np.zeros((len(sentences), 10))

    sent_no = 0

    for s in sentences:
        #tokenize the sentence
        words = word_tokenize(s)
        #create a sentence vector of dimension 300
        sentVector = np.zeros(10)
        #filter out the words in sentences which not belong to stop word category
        filteredSentence = [w for w in words if not w in stop_words] 
        #for each word of the sentence create a word2Vec and add each one to form a sentence vector

        sentVector2 = np.zeros(10)
        ##from lda model get sent vec
        vec_bow = dictionary.doc2bow(filteredSentence)
        sentVector = ldamodel[vec_bow] 
        sentVector = np.array(sentVector) 

        availabldim = []
        for j in sentVector :
            sentVector2[int(j[0])] = j[1]   
        sent_matrix[sent_no,:] = sentVector2
        #Iterate over the next sentence
        sent_no += 1
    filename = file.split("\\")
    filename = filename[len(filename)-1]
    graph_file = open(".\\graphs\\"+filename, 'wb')
    for nodeSource in range(0, (len(sentences)-1)):
        for nodeDest in range(0, (len(sentences)-1)):
            node1 = sent_matrix[nodeSource:nodeSource+1]
            node2 = sent_matrix[nodeDest:nodeDest+1]
            node1 = list(node1[0])
            node2 = list(node2[0])
            cosine_similarity = 1 - spatial.distance.cosine(node1, node2)
            if(math.isnan(cosine_similarity)):
                continue
            modified_similaity = int(cosine_similarity*100)
            print(nodeSource, nodeDest, modified_similaity)
            graph_file.write(str(nodeSource))
            graph_file.write(" ")
            graph_file.write(str(nodeDest))
            graph_file.write(" ")
            graph_file.write(str(modified_similaity))
            graph_file.write("\n")
        
    graph_file.close()
    count = count + 1
